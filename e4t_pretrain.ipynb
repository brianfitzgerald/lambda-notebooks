{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R6mdW3zHog7F",
        "outputId": "09897050-dd93-4be3-8c65-6d1379f56fd0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/e4t-diffusion\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/content/e4t-diffusion/pretrained-wikiart/latest is already a clone of https://huggingface.co/roborovski/e4t-style-concept. Make sure you pull the latest changes with `repo.git_pull()`.\n",
            "WARNING:huggingface_hub.repository:/content/e4t-diffusion/pretrained-wikiart/latest is already a clone of https://huggingface.co/roborovski/e4t-style-concept. Make sure you pull the latest changes with `repo.git_pull()`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token is valid.\n",
            "Your token has been saved in your configured git credential helpers (store).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Checked out style-run-2 from style-run-2.\n",
            "WARNING:huggingface_hub.repository:Checked out style-run-2 from style-run-2.\n",
            "\n",
            "WARNING:huggingface_hub.repository:\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "if not os.path.exists(\"/content/e4t-diffusion\"):\n",
        "  !git clone https://github.com/mkshing/e4t-diffusion.git\n",
        "  !pip install datasets huggingface_hub\n",
        "  !git lfs install\n",
        "%cd e4t-diffusion\n",
        "!pip install -r requirements.txt -q\n",
        "\n",
        "\n",
        "from huggingface_hub import notebook_login, Repository, login\n",
        "\n",
        "login(\"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\", add_to_git_credential=True)\n",
        "\n",
        "repo_name = f\"roborovski/e4t-style-concept\"\n",
        "checkpoint_dir = \"/content/e4t-diffusion/pretrained-wikiart/latest\"\n",
        "epoch = \"50000\"\n",
        "epoch_dir = f\"/content/e4t-diffusion/pretrained-wikiart/{epoch}\"\n",
        "run_name = \"style-run-2\"\n",
        "repo = Repository(clone_from=repo_name, local_dir=checkpoint_dir, repo_type=\"model\", revision=run_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5SZpFQf69DM"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "dWbYoi-rpuun",
        "outputId": "7574edaf-14db-40ad-c24f-3ac771c372ad"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Namespace(pretrained_model_name_or_path='runwayml/stable-diffusion-v1-5', clip_model_name_or_path='ViT-H-14::laion2b_s32b_b79k', placeholder_token='*s', domain_class_token='art', domain_embed_scale=0.1, reg_lambda=0.01, prompt_template='art', train_image_dataset='Artificio/WikiArt', unfreeze_clip_vision=True, webdataset=False, iterable_dataset=True, resolution=512, seed=42, gradient_accumulation_steps=1, max_grad_norm=1.0, learning_rate=1e-06, scale_lr=True, train_batch_size=4, num_train_epochs=1, max_train_steps=100000, dataloader_num_workers=0, checkpointing_steps=10000, resume_from_checkpoint=None, log_steps=1000, enable_xformers_memory_efficient_attention=True, save_sample_prompt='a photo of the *s,a photo of the *s in monet style,a photo of the *s in picasso style', n_save_sample=4, save_guidance_scale=7.5, save_inference_steps=50, report_to='wandb', revision=None, output_dir='pretrained-wikiart', logging_dir='logs', mixed_precision='fp16', use_8bit_adam=False, lr_scheduler='constant', lr_warmup_steps=0, local_rank=-1)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from IPython.display import display\n",
        "import os\n",
        "import argparse\n",
        "import random\n",
        "\n",
        "from packaging import version\n",
        "import math\n",
        "import json\n",
        "from tqdm.auto import tqdm\n",
        "import blobfile as bf\n",
        "import itertools\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import albumentations\n",
        "from einops import rearrange\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import braceexpand\n",
        "import webdataset as wds\n",
        "import wandb\n",
        "from accelerate import Accelerator\n",
        "from accelerate.utils import set_seed\n",
        "from datasets import load_dataset\n",
        "from transformers import CLIPTokenizer\n",
        "from diffusers import DDPMScheduler, AutoencoderKL, DDIMScheduler, DPMSolverMultistepScheduler\n",
        "from diffusers.utils.import_utils import is_xformers_available\n",
        "from diffusers.optimization import get_scheduler\n",
        "from e4t.models.modeling_clip import CLIPTextModel\n",
        "from e4t.encoder import E4TEncoder\n",
        "from e4t.pipeline_stable_diffusion_e4t import StableDiffusionE4TPipeline\n",
        "from e4t.utils import load_e4t_unet, load_e4t_encoder, save_e4t_unet, save_e4t_encoder, image_grid\n",
        "\n",
        "templates = [\n",
        "    \"a photo of {placeholder_token}\",\n",
        "    \"the photo of {placeholder_token}\",\n",
        "    \"a photo of a {placeholder_token}\",\n",
        "    \"a photo of the {placeholder_token}\",\n",
        "    \"a photo of one {placeholder_token}\",\n",
        "    \"a close-up photo of the {placeholder_token}\",\n",
        "    \"a bright photo of the {placeholder_token}\",\n",
        "    \"a photo of a nice {placeholder_token}\",\n",
        "    \"a good photo of {placeholder_token}\",\n",
        "    \"a photo of a cool {placeholder_token}\"\n",
        "]\n",
        "\n",
        "face_templates = templates + [\n",
        "    \"a portrait of {placeholder_token}\",\n",
        "    \"the portrait of {placeholder_token}\",\n",
        "    \"a portrait photo of {placeholder_token}\",\n",
        "    \"portrait of {placeholder_token}\",\n",
        "    \"portrait of the {placeholder_token}\",\n",
        "    \"photo realistic portrait of {placeholder_token}\",\n",
        "]\n",
        "\n",
        "art_templates = templates + [\n",
        "    \"art of {placeholder_token}\",\n",
        "    \"art by {placeholder_token}\",\n",
        "    \"art in the style of {placeholder_token}\",\n",
        "    \"a work of art in the style of {placeholder_token}\",\n",
        "    \"a work by {placeholder_token}\"\n",
        "]\n",
        "\n",
        "\n",
        "def _list_image_files_recursively(data_dir):\n",
        "    results = []\n",
        "    for entry in sorted(bf.listdir(data_dir)):\n",
        "        full_path = bf.join(data_dir, entry)\n",
        "        ext = entry.split(\".\")[-1]\n",
        "        if \".\" in entry and ext.lower() in [\"jpg\", \"jpeg\", \"png\", \"gif\"]:\n",
        "            results.append(full_path)\n",
        "        elif bf.isdir(full_path):\n",
        "            results.extend(_list_image_files_recursively(full_path))\n",
        "    return results\n",
        "\n",
        "\n",
        "def make_transforms(size, random_crop=False):\n",
        "    rescaler = albumentations.SmallestMaxSize(max_size=size, interpolation=3)\n",
        "    if not random_crop:\n",
        "        cropper = albumentations.CenterCrop(height=size, width=size)\n",
        "    else:\n",
        "        cropper = albumentations.RandomCrop(height=size, width=size)\n",
        "    flip = albumentations.HorizontalFlip(p=0.5)\n",
        "    return albumentations.Compose([rescaler, cropper, flip])\n",
        "\n",
        "\n",
        "class E4TDataset(Dataset):\n",
        "    def __init__(\n",
        "            self,\n",
        "            dataset_name,\n",
        "            resolution=512,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        from_datasets = False\n",
        "        if os.path.isdir(dataset_name) or \"::\" in dataset_name:\n",
        "            self.dataset = []\n",
        "            for name in dataset_name.split(\"::\"):\n",
        "                self.dataset += _list_image_files_recursively(name)\n",
        "        else:\n",
        "            self.dataset = load_dataset(dataset_name, split=\"train\")\n",
        "            from_datasets = True\n",
        "        self.from_datasets = from_datasets\n",
        "        self.processor = make_transforms(resolution, random_crop=True)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.dataset[idx]\n",
        "        if self.from_datasets:\n",
        "            image = image[\"image\"]\n",
        "        else:\n",
        "            image = Image.open(image)\n",
        "        image = np.array(image.convert(\"RGB\"))\n",
        "        image = self.processor(image=image)[\"image\"]\n",
        "        image = (image / 127.5 - 1.0).astype(np.float32)\n",
        "        image = torch.from_numpy(image).permute(2, 0, 1)\n",
        "        return dict(\n",
        "            pixel_values=image,\n",
        "        )\n",
        "\n",
        "\n",
        "def get_dataset_size(shards):\n",
        "    shards_list = []\n",
        "    for s in shards.split(\"::\"):\n",
        "        shards_list += list(braceexpand.braceexpand(s))\n",
        "    dir_path = os.path.dirname(shards)\n",
        "    sizes_filename = os.path.join(dir_path, 'sizes.json')\n",
        "    if os.path.exists(sizes_filename):\n",
        "        sizes = json.load(open(sizes_filename, 'r'))\n",
        "        # if \"total_size\" in sizes.keys():\n",
        "        #     total_size = sizes['total_size']\n",
        "        # else:\n",
        "        #     total_size = sum([int(sizes[os.path.basename(shard)]) for shard in shards_list])\n",
        "        total_size = sum([int(sizes[os.path.basename(shard)]) for shard in shards_list])\n",
        "    elif os.path.join(shards_list[0].replace('.tar', \"_stats.json\")):\n",
        "        total_size = 0\n",
        "        for shard in shards_list:\n",
        "            json_path = shard.replace('.tar', \"_stats.json\")\n",
        "            if os.path.exists(json_path):\n",
        "                sizes = json.load(open(json_path))\n",
        "                if 'n_data' in sizes:\n",
        "                    size = sizes['n_data']\n",
        "                else:\n",
        "                    size = sizes[\"successes\"]\n",
        "                total_size += int(size)\n",
        "            else:\n",
        "                print(f\"Not Found {json_path}\")\n",
        "    else:\n",
        "        total_size = None  # num samples undefined\n",
        "    num_shards = len(shards_list)\n",
        "    return total_size, num_shards\n",
        "\n",
        "\n",
        "def filter_webdataset(example):\n",
        "    if \"jpg\" not in example or example[\"jpg\"] is None:\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser(description=\"Simple example of a training script.\")\n",
        "    # e4t configs\n",
        "    parser.add_argument(\"--pretrained_model_name_or_path\", type=str, default=\"runwayml/stable-diffusion-v1-5\", required=False, help=\"Path to pretrained model or model identifier from huggingface.co/models.\",)\n",
        "    parser.add_argument(\"--clip_model_name_or_path\", type=str, default=\"ViT-H-14::laion2b_s32b_b79k\", required=False, help=\"load from open_clip with the format 'arch::version'\")\n",
        "    parser.add_argument(\"--placeholder_token\", type=str, default=\"*s\", help=\"A token to use as a placeholder for the concept.\",)\n",
        "    parser.add_argument(\"--domain_class_token\", type=str, default=None, required=True, help=\"Coarse-class token such as `face`, `cat`, pr `art`\")\n",
        "    parser.add_argument(\"--domain_embed_scale\", type=float, default=0.1, help=\"scale of e4t encoder's embedding\")\n",
        "    parser.add_argument(\"--reg_lambda\", type=float, default=0.01, help=\"l2 regularization lambda\")\n",
        "    parser.add_argument(\"--prompt_template\", type=str, default=\"a photo of {placeholder_token}\", help=\"{placeholder_token} will be replaced to placeholder_token. If you choose from ['normal', 'face', 'art'],use default multiple templates\")\n",
        "    parser.add_argument(\"--train_image_dataset\", type=str, default=None, required=True,\n",
        "                        help=\"A folder containing the training data.\")\n",
        "    parser.add_argument(\"--unfreeze_clip_vision\", action=\"store_true\", default=False, help=\"train clip image encoder as a part of e4t encoder\")\n",
        "    parser.add_argument(\"--webdataset\", action=\"store_true\", default=False, help=\"load tar files via webdataset\")\n",
        "    parser.add_argument(\"--iterable_dataset\", action=\"store_true\", default=False, help=\"Use iterable dataset in datasets\")\n",
        "    # training\n",
        "    parser.add_argument(\"--resolution\", type=int, default=512)\n",
        "    parser.add_argument(\"--seed\", type=int, default=42, help=\"A seed for reproducible training.\")\n",
        "    parser.add_argument(\"--gradient_accumulation_steps\", type=int, default=1, help=\"Number of updates steps to accumulate before performing a backward/update pass.\",)\n",
        "    parser.add_argument(\"--max_grad_norm\", default=1.0, type=float, help=\"Max gradient norm.\")\n",
        "    parser.add_argument(\"--learning_rate\", type=float, default=1.6e-5, help=\"learning rate\",)\n",
        "    parser.add_argument(\"--scale_lr\", action=\"store_true\", default=False, help=\"Scale the learning rate by the number of GPUs, gradient accumulation steps, and batch size.\")\n",
        "    parser.add_argument(\"--train_batch_size\", type=int, default=16, help=\"Batch size (per device) for the training dataloader.\")\n",
        "    parser.add_argument(\"--num_train_epochs\", type=int, default=1,)\n",
        "    parser.add_argument(\"--max_train_steps\", type=int, default=30000, help=\"Total number of training steps to perform. For face, 30,000. For cat, 60,000. For art, 100,000\",)\n",
        "    parser.add_argument(\"--dataloader_num_workers\", type=int, default=0, help=\"Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process.\")\n",
        "    parser.add_argument(\"--checkpointing_steps\", type=int, default=10000, help=\"Save a checkpoint of the training state every X updates.\")\n",
        "    parser.add_argument(\"--resume_from_checkpoint\", type=str, default=None, help=(\"Whether training should be resumed from a previous checkpoint. Use a path saved by\"\n",
        "        ' `--checkpointing_steps`, or `\"latest\"` to automatically select the last available checkpoint.'\n",
        "    ))\n",
        "    parser.add_argument(\"--log_steps\", type=int, default=1000, help=\"sample images \")\n",
        "    parser.add_argument(\"--enable_xformers_memory_efficient_attention\", action=\"store_true\", help=\"Whether or not to use xformers.\")\n",
        "    # log\n",
        "    parser.add_argument(\"--save_sample_prompt\", type=str, default=\"a photo of *s,a photo of *s in the style of monet\", help=\"split with ',' for multiple prompts\")\n",
        "    parser.add_argument(\"--n_save_sample\", type=int, default=4, help=\"The number of samples per prompt\")\n",
        "    parser.add_argument(\"--save_guidance_scale\", type=float, default=7.5, help=\"CFG for save sample.\")\n",
        "    parser.add_argument(\"--save_inference_steps\", type=int, default=50, help=\"The number of inference steps for save sample.\",)\n",
        "    # general\n",
        "    parser.add_argument(\"--report_to\", type=str, default=\"wandb\", choices=[\"tensorboard\", \"wandb\"])\n",
        "    parser.add_argument(\"--revision\", type=str, default=None, required=False, help=\"Revision of pretrained model identifier from huggingface.co/models.\", )\n",
        "    parser.add_argument(\"--output_dir\", type=str, default=\"e4t-model\", help=\"The output directory where the model predictions and checkpoints will be written.\", )\n",
        "    parser.add_argument(\"--logging_dir\", type=str, default=\"logs\")\n",
        "    parser.add_argument(\"--mixed_precision\", type=str, default=\"no\", choices=[\"no\", \"fp16\", \"bf16\"])\n",
        "    parser.add_argument(\"--use_8bit_adam\", action=\"store_true\", help=\"Whether or not to use 8-bit Adam from bitsandbytes.\")\n",
        "    parser.add_argument(\"--lr_scheduler\", type=str, default=\"constant\", help='The scheduler type to use. Choose between [\"linear\", \"cosine\", \"cosine_with_restarts\", \"polynomial\", \"constant\", \"constant_with_warmup\"]')\n",
        "    parser.add_argument(\"--lr_warmup_steps\", type=int, default=0, help=\"Number of steps for the warmup in the lr scheduler.\")\n",
        "    parser.add_argument(\"--local_rank\", type=int, default=-1, help=\"For distributed training: local_rank\")\n",
        "    args = parser.parse_args(['--domain_class_token', 'art', '--train_image_dataset', 'Artificio/WikiArt'])\n",
        "    env_local_rank = int(os.environ.get(\"LOCAL_RANK\", -1))\n",
        "    if env_local_rank != -1 and env_local_rank != args.local_rank:\n",
        "        args.local_rank = env_local_rank\n",
        "\n",
        "    if args.train_image_dataset is None:\n",
        "        raise ValueError(\"You must specify a train data directory.\")\n",
        "    if args.domain_class_token is None:\n",
        "        raise ValueError(\"You must specify a coarse-class token.\")\n",
        "    return args\n",
        "\n",
        "args = parse_args()\n",
        "\n",
        "args.pretrained_model_name_or_path=\"runwayml/stable-diffusion-v1-5\" \n",
        "checkpoint_dir = \"/content/e4t-diffusion/pretrained-wikiart/latest/\" \n",
        "# args.resume_from_checkpoint = checkpoint_dir\n",
        "args.clip_model_name_or_path=\"ViT-H-14::laion2b_s32b_b79k\" \n",
        "args.domain_class_token=\"art\" \n",
        "args.placeholder_token=\"*s\" \n",
        "args.prompt_template=\"art\" \n",
        "args.save_sample_prompt=\"a photo of the *s,a photo of the *s in monet style,a photo of the *s in picasso style\" \n",
        "args.reg_lambda=0.01 \n",
        "args.domain_embed_scale=0.1 \n",
        "args.output_dir=\"pretrained-wikiart\" \n",
        "args.train_image_dataset=\"Artificio/WikiArt\" \n",
        "args.iterable_dataset = True\n",
        "args.resolution=512 \n",
        "args.train_batch_size=4 \n",
        "args.learning_rate=1e-6\n",
        "args.scale_lr=True\n",
        "args.checkpointing_steps=10000 \n",
        "args.log_steps=1000 \n",
        "args.max_train_steps=100000 \n",
        "args.unfreeze_clip_vision = True\n",
        "args.mixed_precision=\"fp16\" \n",
        "\n",
        "args.enable_xformers_memory_efficient_attention = True\n",
        "display(args)\n",
        "\n",
        "# change e4t/utils.py:94 to 'if pretrained_model_name_or_path is None:'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzCfiQcio-Tb",
        "outputId": "49ea104d-8286-467f-a178-121e3b6ec6ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model name:  runwayml/stable-diffusion-v1-5\n",
            "clip name:  ViT-H-14::laion2b_s32b_b79k\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/modeling_utils.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  with safe_open(checkpoint_file, framework=\"pt\") as f:\n",
            "/usr/local/lib/python3.9/dist-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  return self.fget.__get__(instance, owner)()\n",
            "/usr/local/lib/python3.9/dist-packages/torch/storage.py:899: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  storage = cls(wrap_storage=untyped_storage)\n",
            "/usr/local/lib/python3.9/dist-packages/safetensors/torch.py:99: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  with safe_open(filename, framework=\"pt\", device=device) as f:\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pretrained model runwayml/stable-diffusion-v1-5\n",
            "offsets path None\n",
            "Number of Trainable Parameters: 1005.34 M\n"
          ]
        }
      ],
      "source": [
        "logging_dir = os.path.join(args.output_dir, args.logging_dir)\n",
        "accelerator = Accelerator(\n",
        "    gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
        "    mixed_precision=args.mixed_precision,\n",
        "    log_with=args.report_to,\n",
        "    project_dir=logging_dir,\n",
        ")\n",
        "if args.seed is not None:\n",
        "    set_seed(args.seed)\n",
        "\n",
        "print('model name: ', args.pretrained_model_name_or_path)\n",
        "print('clip name: ', args.clip_model_name_or_path)\n",
        "\n",
        "# load pre-trained model\n",
        "tokenizer = CLIPTokenizer.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"tokenizer\")\n",
        "noise_scheduler = DDPMScheduler.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"scheduler\")\n",
        "text_encoder = CLIPTextModel.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"text_encoder\", revision=args.revision)\n",
        "vae = AutoencoderKL.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"vae\", revision=args.revision)\n",
        "offsets_ckpt_path = os.path.join(args.pretrained_model_name_or_path, \"weight_offsets.pt\") if os.path.exists(os.path.join(args.pretrained_model_name_or_path, \"weight_offsets.pt\")) else None\n",
        "print('pretrained model', args.pretrained_model_name_or_path)\n",
        "print('offsets path', offsets_ckpt_path)\n",
        "unet = load_e4t_unet(\n",
        "    args.pretrained_model_name_or_path, \n",
        "    ckpt_path=offsets_ckpt_path,\n",
        "    revision=args.revision\n",
        ")\n",
        "# encoder\n",
        "e4t_encoder = load_e4t_encoder(\n",
        "    word_embedding_dim=text_encoder.config.hidden_size,\n",
        "    block_out_channels=unet.config.block_out_channels,\n",
        "    arch=args.clip_model_name_or_path.split(\"::\")[0],\n",
        "    version=args.clip_model_name_or_path.split(\"::\")[1],\n",
        "    freeze_clip_vision=not args.unfreeze_clip_vision,\n",
        "    ckpt_path=os.path.join(args.pretrained_model_name_or_path, \"encoder.pt\") if os.path.exists(os.path.join(args.pretrained_model_name_or_path, \"weight_offsets.pt\")) else None,\n",
        ")\n",
        "\n",
        "# Add the placeholder token in tokenizer\n",
        "num_added_tokens = tokenizer.add_tokens(args.placeholder_token)\n",
        "if num_added_tokens == 0:\n",
        "    raise ValueError(f\"The tokenizer already contains the token {args.placeholder_token}. Please pass a different `placeholder_token` that is not already in the tokenizer.\")\n",
        "placeholder_token_id = tokenizer.convert_tokens_to_ids(args.placeholder_token)\n",
        "# Resize the token embeddings as we are adding new special tokens to the tokenizer\n",
        "text_encoder.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# freeze\n",
        "vae.requires_grad_(False)\n",
        "text_encoder.requires_grad_(False)\n",
        "if is_xformers_available() and args.enable_xformers_memory_efficient_attention:\n",
        "    import xformers\n",
        "    xformers_version = version.parse(xformers.__version__)\n",
        "    if xformers_version == version.parse(\"0.0.16\"):\n",
        "        print(\"[WARNING] xFormers 0.0.16 cannot be used for training in some GPUs. If you observe problems during training, please update xFormers to at least 0.0.17. See https://huggingface.co/docs/diffusers/main/en/optimization/xformers for more details.\")\n",
        "    unet.enable_xformers_memory_efficient_attention()\n",
        "    print(\"Using xFormers!\")\n",
        "# else:\n",
        "#     raise ValueError(\"xformers is not available. Make sure it is installed correctly\")\n",
        "# Initialize the optimizer\n",
        "optim_params = [p for p in e4t_encoder.parameters() if p.requires_grad]\n",
        "# weight offsets\n",
        "for n, p in unet.named_parameters():\n",
        "    if \"wo\" in n:\n",
        "        optim_params += [p]\n",
        "total_params = sum(p.numel() for p in optim_params)\n",
        "print(f\"Number of Trainable Parameters: {total_params * 1.e-6:.2f} M\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "e324fd6012c94facb7c3d80afb5af639",
            "cc332469a6634e949b8a75602b3e3417",
            "4f600f841743448c88322a4002316f5d",
            "ffa71bf2e2064a678c14e914de7bcd84",
            "9ab5463c284e4ca8935a1bad31d7384d",
            "cdb633a5f96645fbbb805b447d3fa59e",
            "a81cbf0e261d434d8e8ac93a91b4ce64",
            "feeb134461c6429d8a6c8cbb0a732953",
            "4f98069d1e924e10bba82dc7cabf40f8",
            "25ca156f7b9941009ac187bc3174b9d2",
            "fa5bcd6c12b042648b853ce201eb73f4",
            "83e8730cdbb1457aa25238cc03d08ecd",
            "69dd3170dae14ed281f1973beead8080",
            "b957906aa6aa476e895f9bc9a223a05e",
            "4c5f501869bb4cd78114cbc3d551513c",
            "a2017d8f67de41528628e6511107a3cc",
            "42a5063b02a34ac286dbf7d6d5282448",
            "8edcf0e5d7ac496c8945bbd42f5b6008",
            "644bd4ef6df645e18c53e382431226f3",
            "0e5e18c42a4545a4bc46b04fbf07925e",
            "2cc85a5042c84988b18365e371a47e06",
            "d9fd0c54eb93406a9dcefe8df1d0f1e8",
            "7d3870e72a2a414d851210ff06290b08",
            "e2db3b7756e34800bc96d9c9607145e4",
            "5d40623c7a974fbeb7b4bab9195baa05",
            "7d8d5fab1be147d78adc971d8725b698",
            "49498b5aed434d89ab552f865ba7f90b",
            "12a91a7416884a049b630a505aaaf21e",
            "85719fcb084949cb94ea651b89f8d31a",
            "4dfcae0052c14c72ac3d80244f3d1c70",
            "b9cfe56975a34edb9d163685f84a1e89",
            "bce8b02a88264b948b816a72758960fc",
            "6690d2ba289546238836ec279e1101c0",
            "9de20d8dbd724797bda81511174b5b83",
            "977f9c56bc4745389cd25dbece1d7943",
            "5aabd49f9598483c88bd706faeee47ae",
            "4337971a74814b5aa57bf2e97dfd34cc",
            "ad392b8f7a294d9299951cc105090b25",
            "7c0f2413fe9840d7b031499ade888af9",
            "21cfc8869f194adfafc97c00f9c07736",
            "9bcbf0f583784090831a72f1df67f850",
            "9793864716814310b2aba68c55290df1",
            "41417068b4fe404fb8783021b2c436f7",
            "c9734eee8617490c8f54b7284fe93b37",
            "01f8c09ae9ba468896b25d3c2daf17fa",
            "0d11b6b3544242178b5bea30ba3f72b4",
            "ac8deae4b3a84a6e8d180ca0653f699e",
            "9c5a02eae98a4084a2408b0a9d589b65",
            "47f09e7af54d417faf2ef8fb598cc6f4",
            "90be63ad618940f796801a9e93ff5478",
            "48eda0ddbdde46059e1a1bd0c007695d",
            "9c161d13732248fd81926085b8f76825",
            "f653dc8c87e447ffbb13ac130b352791",
            "c2b623d3d0874499a0610e8f35934f11",
            "1f5faac2a7e543de8e9f04eeee101b0f",
            "3aef37a4593e41d4845effcd9f9c7dde",
            "72fde63415c343fb98a135ae671d4471",
            "a5c01fc06f5e485aab2f708646212ab4",
            "d85ca962a72444fc81da198035789a26",
            "cf75c9b686fb4ccea0a04d98b977b6cc",
            "6c20c1bd9b4f44a482730d51a7f9d91d",
            "c7e7f21c262348e39df02bfb2db7cbf3",
            "d4f2932bddb54b4baec49508f5c3285a",
            "9e425aa15d214bbdb984e82c1b04d774",
            "6b779252ae674654b366966f4a0f7ef5",
            "a60e4b66ee59436d9ab1d35837d56688",
            "149341838cd4463486eed27a528341cd",
            "7dd22145eac14046a50662f8ab36f4af",
            "1873e0b52291426cb1f9f1e1c2102ef6",
            "73aa20ca888944fea074a2e0a19a3bda",
            "80ce3827ea4f4da8aa578161ce33bbc3",
            "076d81c9c6fb4045bc25df316ac823c6",
            "8cd32136d94343d1948b26fddd43c17e",
            "6e5e4f2331d7419f8cf7d8352f7a57b5",
            "887f39a6f38540b09b3b5c61fe85c18a",
            "25215c39715a4451b07327545e9f631a",
            "ad2fe2e38d5847d8a4a5c5900c56e1de",
            "eccad653763c408e90a6dc5925b9e722",
            "52f911b47497447fa514097875beaf8e",
            "55a1f1229fef4fdaa3d62a80c1f98c89"
          ]
        },
        "id": "CzQADc4ucDNE",
        "outputId": "688e8afb-8558-41ef-af71-a56151929bb9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting learning rate to 4.00e-06 = 1 (accumulate_grad_batches) * 1 (num_gpus) * 4 (batchsize) * 1.00e-06 (base_lr)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbfitzgerald\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.14.2"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/e4t-diffusion/wandb/run-20230417_004115-2ysnmmq6</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/bfitzgerald/e4t/runs/2ysnmmq6' target=\"_blank\">royal-pond-14</a></strong> to <a href='https://wandb.ai/bfitzgerald/e4t' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/bfitzgerald/e4t' target=\"_blank\">https://wandb.ai/bfitzgerald/e4t</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/bfitzgerald/e4t/runs/2ysnmmq6' target=\"_blank\">https://wandb.ai/bfitzgerald/e4t/runs/2ysnmmq6</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "***** Running training *****\n",
            "  Instantaneous batch size per device = 4\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 100000\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e324fd6012c94facb7c3d80afb5af639",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using the default 15 templates!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "83e8730cdbb1457aa25238cc03d08ecd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating samples:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7d3870e72a2a414d851210ff06290b08",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating samples:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e2db3b7756e34800bc96d9c9607145e4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating samples:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5d40623c7a974fbeb7b4bab9195baa05",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating samples:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7d8d5fab1be147d78adc971d8725b698",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating samples:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "49498b5aed434d89ab552f865ba7f90b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating samples:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "12a91a7416884a049b630a505aaaf21e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating samples:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "85719fcb084949cb94ea651b89f8d31a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating samples:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4dfcae0052c14c72ac3d80244f3d1c70",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating samples:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b9cfe56975a34edb9d163685f84a1e89",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating samples:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[*] Weights saved at pretrained-wikiart/10000\n",
            "Saved state to pretrained-wikiart/checkpoint-10000\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bce8b02a88264b948b816a72758960fc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating samples:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6690d2ba289546238836ec279e1101c0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating samples:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9de20d8dbd724797bda81511174b5b83",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating samples:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "977f9c56bc4745389cd25dbece1d7943",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating samples:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5aabd49f9598483c88bd706faeee47ae",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating samples:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4337971a74814b5aa57bf2e97dfd34cc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating samples:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ad392b8f7a294d9299951cc105090b25",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating samples:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7c0f2413fe9840d7b031499ade888af9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating samples:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "21cfc8869f194adfafc97c00f9c07736",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating samples:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9bcbf0f583784090831a72f1df67f850",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating samples:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[*] Weights saved at pretrained-wikiart/20000\n",
            "Saved state to pretrained-wikiart/checkpoint-20000\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9793864716814310b2aba68c55290df1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating samples:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "41417068b4fe404fb8783021b2c436f7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating samples:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c9734eee8617490c8f54b7284fe93b37",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating samples:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "01f8c09ae9ba468896b25d3c2daf17fa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating samples:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0d11b6b3544242178b5bea30ba3f72b4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating samples:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ac8deae4b3a84a6e8d180ca0653f699e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating samples:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9c5a02eae98a4084a2408b0a9d589b65",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating samples:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "47f09e7af54d417faf2ef8fb598cc6f4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating samples:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "90be63ad618940f796801a9e93ff5478",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating samples:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "48eda0ddbdde46059e1a1bd0c007695d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating samples:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[*] Weights saved at pretrained-wikiart/30000\n",
            "Saved state to pretrained-wikiart/checkpoint-30000\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9c161d13732248fd81926085b8f76825",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating samples:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f653dc8c87e447ffbb13ac130b352791",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating samples:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c2b623d3d0874499a0610e8f35934f11",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating samples:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1f5faac2a7e543de8e9f04eeee101b0f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating samples:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3aef37a4593e41d4845effcd9f9c7dde",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating samples:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "72fde63415c343fb98a135ae671d4471",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating samples:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a5c01fc06f5e485aab2f708646212ab4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating samples:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d85ca962a72444fc81da198035789a26",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating samples:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cf75c9b686fb4ccea0a04d98b977b6cc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating samples:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6c20c1bd9b4f44a482730d51a7f9d91d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating samples:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[*] Weights saved at pretrained-wikiart/40000\n",
            "Saved state to pretrained-wikiart/checkpoint-40000\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c7e7f21c262348e39df02bfb2db7cbf3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating samples:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d4f2932bddb54b4baec49508f5c3285a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating samples:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9e425aa15d214bbdb984e82c1b04d774",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating samples:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6b779252ae674654b366966f4a0f7ef5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating samples:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a60e4b66ee59436d9ab1d35837d56688",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating samples:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "149341838cd4463486eed27a528341cd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating samples:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7dd22145eac14046a50662f8ab36f4af",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating samples:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1873e0b52291426cb1f9f1e1c2102ef6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating samples:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "73aa20ca888944fea074a2e0a19a3bda",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating samples:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "80ce3827ea4f4da8aa578161ce33bbc3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating samples:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[*] Weights saved at pretrained-wikiart/50000\n",
            "Saved state to pretrained-wikiart/checkpoint-50000\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "076d81c9c6fb4045bc25df316ac823c6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating samples:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8cd32136d94343d1948b26fddd43c17e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating samples:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6e5e4f2331d7419f8cf7d8352f7a57b5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating samples:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "887f39a6f38540b09b3b5c61fe85c18a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating samples:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "25215c39715a4451b07327545e9f631a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating samples:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ad2fe2e38d5847d8a4a5c5900c56e1de",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating samples:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eccad653763c408e90a6dc5925b9e722",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating samples:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "52f911b47497447fa514097875beaf8e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating samples:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "55a1f1229fef4fdaa3d62a80c1f98c89",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating samples:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Only load unet, but restart the whole training process\n",
        "resume_unet_only = False\n",
        "\n",
        "# dataset\n",
        "if not args.iterable_dataset and not args.webdataset:\n",
        "    train_dataset = E4TDataset(\n",
        "        dataset_name=args.train_image_dataset,\n",
        "        resolution=args.resolution,\n",
        "    )\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=args.train_batch_size, shuffle=True, num_workers=args.dataloader_num_workers)\n",
        "else:\n",
        "    processor = make_transforms(args.resolution, random_crop=True)\n",
        "\n",
        "    def preprocess(example):\n",
        "        image_key = \"image\" if not args.webdataset else \"jpg\"\n",
        "        image = np.array(example[image_key]).astype(np.uint8)\n",
        "        image = processor(image=image)[\"image\"]\n",
        "        image = (image / 127.5 - 1.0).astype(np.float32)\n",
        "        image = torch.from_numpy(image).permute(2, 0, 1)\n",
        "        return image\n",
        "\n",
        "    def collate_fn(examples):\n",
        "        image = torch.stack([preprocess(example) for example in examples])\n",
        "        return dict(pixel_values=image)\n",
        "\n",
        "    if args.webdataset:\n",
        "        num_samples, num_shards = get_dataset_size(args.train_image_dataset)\n",
        "        print(f'Loading webdataset with {num_shards} shards. (num_samples: {num_samples})')\n",
        "        pipeline = [wds.ResampledShards(args.train_image_dataset)]\n",
        "        pipeline.extend([\n",
        "            wds.split_by_node,\n",
        "            wds.split_by_worker,\n",
        "            # at this point, we have an iterator over the shards assigned to each worker at each node\n",
        "            wds.tarfile_to_samples(handler=wds.warn_and_continue),\n",
        "            wds.shuffle(1000, handler=wds.warn_and_continue),\n",
        "        ])\n",
        "        pipeline.extend([\n",
        "            wds.select(filter_webdataset),\n",
        "            wds.decode(\"pilrgb\", handler=wds.warn_and_continue),\n",
        "            wds.map(preprocess, handler=wds.warn_and_continue),\n",
        "            # wds.batched(args.train_batch_size, partial=False, collation_fn=lambda images: dict(pixel_values=torch.stack(images))),\n",
        "        ])\n",
        "        train_dataset = wds.DataPipeline(*pipeline)\n",
        "        world_size = accelerator.num_processes\n",
        "        assert num_shards >= args.dataloader_num_workers * world_size, 'number of shards must be >= total workers'\n",
        "        global_batch_size = args.train_batch_size * world_size\n",
        "        num_batches = math.ceil(num_samples / global_batch_size)\n",
        "        num_worker_batches = math.ceil(num_batches / args.dataloader_num_workers)  # per dataloader worker\n",
        "        num_batches = num_worker_batches * args.dataloader_num_workers\n",
        "        num_samples = num_batches * global_batch_size\n",
        "        train_dataset = train_dataset.with_epoch(num_worker_batches)  # each worker is iterating over this\n",
        "        train_dataloader = DataLoader(\n",
        "            train_dataset,\n",
        "            persistent_workers=True,\n",
        "            drop_last=True,\n",
        "            num_workers=args.dataloader_num_workers,\n",
        "            collate_fn=lambda images: dict(pixel_values=torch.stack(images))\n",
        "        )\n",
        "        # train_dataloader = wds.WebLoader(\n",
        "        #     train_dataset,\n",
        "        #     batch_size=None,\n",
        "        #     shuffle=False,\n",
        "        #     num_workers=args.dataloader_num_workers,\n",
        "        #     persistent_workers=True,\n",
        "        # )\n",
        "        # # add meta-data to dataloader instance for convenience\n",
        "        # train_dataloader.num_batches = num_batches\n",
        "        # train_dataloader.num_samples = num_samples\n",
        "    else:\n",
        "        train_dataset = load_dataset(args.train_image_dataset, split=\"train\", streaming=True)\n",
        "        train_dataset = train_dataset.shuffle(seed=args.seed, buffer_size=10000)\n",
        "        train_dataset = train_dataset.with_format(\"torch\")\n",
        "        train_dataloader = DataLoader(train_dataset, batch_size=args.train_batch_size, collate_fn=collate_fn)\n",
        "\n",
        "if args.scale_lr:\n",
        "    learning_rate = (\n",
        "            args.learning_rate * args.gradient_accumulation_steps * args.train_batch_size * accelerator.num_processes\n",
        "    )\n",
        "    print(\n",
        "        \"Setting learning rate to {:.2e} = {} (accumulate_grad_batches) * {} (num_gpus) * {} (batchsize) * {:.2e} (base_lr)\".format(\n",
        "            learning_rate, args.gradient_accumulation_steps, accelerator.num_processes, args.train_batch_size, args.learning_rate))\n",
        "    args.learning_rate = learning_rate\n",
        "\n",
        "# Check that all trainable models are in full precision\n",
        "low_precision_error_string = (\n",
        "    \"Please make sure to always have all model weights in full float32 precision when starting training - even if\"\n",
        "    \" doing mixed precision training. copy of the weights should still be float32.\"\n",
        ")\n",
        "\n",
        "if accelerator.unwrap_model(unet).dtype != torch.float32:\n",
        "    raise ValueError(\n",
        "        f\"Unet loaded as datatype {accelerator.unwrap_model(unet).dtype}. {low_precision_error_string}\"\n",
        "    )\n",
        "\n",
        "if accelerator.unwrap_model(e4t_encoder).dtype != torch.float32:\n",
        "    raise ValueError(\n",
        "        f\"Text encoder loaded as datatype {accelerator.unwrap_model(e4t_encoder).dtype}.\"\n",
        "        f\" {low_precision_error_string}\"\n",
        "    )\n",
        "# Use 8-bit Adam for lower memory usage or to fine-tune the model in 16GB GPUs\n",
        "if args.use_8bit_adam:\n",
        "    try:\n",
        "        import bitsandbytes as bnb\n",
        "    except ImportError:\n",
        "        raise ImportError(\"To use 8-bit Adam, please install the bitsandbytes library: `pip install bitsandbytes`.\")\n",
        "    optimizer_class = bnb.optim.AdamW8bit\n",
        "else:\n",
        "    optimizer_class = torch.optim.AdamW\n",
        "\n",
        "optimizer = optimizer_class(\n",
        "    optim_params,\n",
        "    lr=args.learning_rate,\n",
        ")\n",
        "\n",
        "# Scheduler and math around the number of training steps.\n",
        "overrode_max_train_steps = False\n",
        "if not args.iterable_dataset and not args.webdataset:\n",
        "    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n",
        "    if args.max_train_steps is None:\n",
        "        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n",
        "        overrode_max_train_steps = True\n",
        "\n",
        "lr_scheduler = get_scheduler(\n",
        "    args.lr_scheduler,\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=args.lr_warmup_steps * args.gradient_accumulation_steps,\n",
        "    num_training_steps=args.max_train_steps * args.gradient_accumulation_steps,\n",
        ")\n",
        "\n",
        "# Prepare everything with our `accelerator`.\n",
        "unet, e4t_encoder, optimizer, lr_scheduler, train_dataloader = accelerator.prepare(\n",
        "    unet, e4t_encoder, optimizer, lr_scheduler, train_dataloader\n",
        ")\n",
        "\n",
        "# For mixed precision training we cast the unet and vae weights to half-precision\n",
        "# as these models are only used for inference, keeping weights in full precision is not required.\n",
        "weight_dtype = torch.float32\n",
        "if accelerator.mixed_precision == \"fp16\":\n",
        "    weight_dtype = torch.float16\n",
        "elif accelerator.mixed_precision == \"bf16\":\n",
        "    weight_dtype = torch.bfloat16\n",
        "# Move vae and unet to device and cast to weight_dtype\n",
        "text_encoder.to(accelerator.device, dtype=weight_dtype)\n",
        "vae.to(accelerator.device, dtype=weight_dtype)\n",
        "\n",
        "if not args.iterable_dataset and not args.webdataset:\n",
        "    # We need to recalculate our total training steps as the size of the training dataloader may have changed.\n",
        "    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n",
        "    if overrode_max_train_steps:\n",
        "        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n",
        "    # Afterwards we recalculate our number of training epochs\n",
        "    args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n",
        "else:\n",
        "    args.num_train_epochs = 1000000000000000000000000000000000000\n",
        "# We need to initialize the trackers we use, and also store our configuration.\n",
        "# The trackers initializes automatically on the main process.\n",
        "if accelerator.is_main_process:\n",
        "    accelerator.init_trackers(\"e4t\", config=vars(args))\n",
        "# Train!\n",
        "total_batch_size = args.train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n",
        "\n",
        "print(\"***** Running training *****\")\n",
        "if not args.iterable_dataset and not args.webdataset:\n",
        "    print(f\"  Num examples = {len(train_dataset)}\")\n",
        "    print(f\"  Num Epochs = {args.num_train_epochs}\")\n",
        "print(f\"  Instantaneous batch size per device = {args.train_batch_size}\")\n",
        "print(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n",
        "print(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n",
        "print(f\"  Total optimization steps = {args.max_train_steps}\")\n",
        "\n",
        "scheduler = DDIMScheduler.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"scheduler\")\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample(images, step):\n",
        "    images_to_log = []\n",
        "    # to pil\n",
        "    x_samples = torch.clamp((images + 1.0) / 2.0, min=0.0, max=1.0)\n",
        "    for x_sample in x_samples:\n",
        "        x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n",
        "        img = Image.fromarray(x_sample.astype(np.uint8))\n",
        "        images_to_log.append(img)\n",
        "    pipeline = StableDiffusionE4TPipeline(\n",
        "        unet=accelerator.unwrap_model(unet, keep_fp32_wrapper=True),\n",
        "        text_encoder=text_encoder,\n",
        "        tokenizer=tokenizer,\n",
        "        vae=vae,\n",
        "        scheduler=scheduler,\n",
        "        e4t_encoder=accelerator.unwrap_model(e4t_encoder, keep_fp32_wrapper=True),\n",
        "        e4t_config=args,\n",
        "        already_added_placeholder_token=True,\n",
        "        requires_safety_checker=False,\n",
        "        safety_checker=None,\n",
        "        feature_extractor=None,\n",
        "    )\n",
        "\n",
        "    if is_xformers_available():\n",
        "        pipeline.enable_xformers_memory_efficient_attention()\n",
        "    pipeline = pipeline.to(accelerator.device)\n",
        "    g_cuda = torch.Generator(device=accelerator.device)\n",
        "    g_cuda = g_cuda.manual_seed(int(g_cuda.seed()))\n",
        "    # g_cuda = torch.Generator(device=accelerator.device).manual_seed(args.seed)\n",
        "    pipeline.set_progress_bar_config(disable=True)\n",
        "    sample_dir = os.path.join(args.output_dir, \"samples\")\n",
        "    os.makedirs(sample_dir, exist_ok=True)\n",
        "    prompts = args.save_sample_prompt.split(\",\")\n",
        "    image_list = []\n",
        "    selected_images_to_log = random.sample(images_to_log, min(len(images_to_log), args.n_save_sample))\n",
        "    with torch.autocast(\"cuda\"), torch.inference_mode():\n",
        "        for save_prompt in tqdm(prompts, desc=\"Generating samples\"):\n",
        "            for image in selected_images_to_log:\n",
        "                images = pipeline(\n",
        "                    save_prompt,\n",
        "                    guidance_scale=args.save_guidance_scale,\n",
        "                    num_inference_steps=args.save_inference_steps,\n",
        "                    generator=g_cuda,\n",
        "                    image=image,\n",
        "                ).images\n",
        "                image_list.append(images[0])\n",
        "    input_grid = image_grid(selected_images_to_log, rows=1, cols=len(selected_images_to_log))\n",
        "    sample_grid = image_grid(image_list, rows=len(prompts), cols=len(selected_images_to_log))\n",
        "    if args.report_to == \"wandb\":\n",
        "        accelerator.log(\n",
        "            {\n",
        "                \"train/inputs\": wandb.Image(input_grid),\n",
        "                \"train/samples\": wandb.Image(sample_grid)\n",
        "            },\n",
        "            step=step\n",
        "        )\n",
        "    else:\n",
        "        input_grid.save(os.path.join(sample_dir, f\"input-{step}.png\"))\n",
        "        sample_grid.save(os.path.join(sample_dir, f\"sample-{step}.png\"))\n",
        "    del pipeline\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "def save_weights(step):\n",
        "    # Create the pipeline using using the trained modules and save it.\n",
        "    if accelerator.is_main_process:\n",
        "        unet_model = accelerator.unwrap_model(unet, keep_fp32_wrapper=True)\n",
        "        e4t_enc_model = accelerator.unwrap_model(e4t_encoder, keep_fp32_wrapper=True)\n",
        "        save_dir = os.path.join(args.output_dir, f\"{step}\")\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "        with open(os.path.join(save_dir, \"config.json\"), \"w\") as f:\n",
        "            json.dump(args.__dict__, f, indent=2)\n",
        "        # save weight offsets\n",
        "        save_e4t_unet(unet_model, save_dir)\n",
        "        # save encoder\n",
        "        save_e4t_encoder(e4t_enc_model, save_dir)\n",
        "        print(f\"[*] Weights saved at {save_dir}\")\n",
        "\n",
        "# Only show the progress bar once on each machine.\n",
        "progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)\n",
        "progress_bar.set_description(\"Steps\")\n",
        "first_epoch = 0\n",
        "global_step = 0\n",
        "# Potentially load in the weights and states from a previous save\n",
        "if args.resume_from_checkpoint and not resume_unet_only:\n",
        "    if args.resume_from_checkpoint != \"latest\":\n",
        "        path = os.path.basename(args.resume_from_checkpoint)\n",
        "    else:\n",
        "        # Get the mos recent checkpoint\n",
        "        dirs = os.listdir(args.output_dir)\n",
        "        dirs = [d for d in dirs if d.startswith(\"checkpoint\")]\n",
        "        dirs = sorted(dirs, key=lambda x: int(x.split(\"-\")[1]))\n",
        "        path = dirs[-1] if len(dirs) > 0 else None\n",
        "\n",
        "    if path is None:\n",
        "        accelerator.print(\n",
        "            f\"Checkpoint '{args.resume_from_checkpoint}' does not exist. Starting a new training run.\"\n",
        "        )\n",
        "        args.resume_from_checkpoint = None\n",
        "    else:\n",
        "        accelerator.print(f\"Resuming from checkpoint {path}\")\n",
        "        accelerator.load_state(os.path.join(args.output_dir, path))\n",
        "        global_step = int(path.split(\"-\")[1])\n",
        "\n",
        "        resume_global_step = global_step * args.gradient_accumulation_steps\n",
        "        first_epoch = global_step // num_update_steps_per_epoch\n",
        "        resume_step = resume_global_step % (num_update_steps_per_epoch * args.gradient_accumulation_steps)\n",
        "\n",
        "if resume_unet_only:\n",
        "  global_step = 50000\n",
        "  resume_global_step = global_step * args.gradient_accumulation_steps\n",
        "  first_epoch = global_step\n",
        "  resume_step = global_step\n",
        "\n",
        "\n",
        "# save class embed\n",
        "domain_class_token_id = tokenizer(args.domain_class_token, add_special_tokens=False, return_tensors=\"pt\").input_ids[0]\n",
        "assert domain_class_token_id.size(0) == 1\n",
        "# get class token embedding\n",
        "class_embed = text_encoder.get_input_embeddings()(domain_class_token_id.to(accelerator.device))\n",
        "input_ids_for_encoder = tokenizer(\n",
        "    \"\",\n",
        "    # args.prompt_template.format(placeholder_token=args.domain_class_token),\n",
        "    padding=\"max_length\", truncation=True, max_length=tokenizer.model_max_length,\n",
        "    return_tensors=\"pt\"\n",
        ").input_ids\n",
        "if args.prompt_template in [\"normal\", \"face\", \"art\"]:\n",
        "    if args.prompt_template == \"normal\":\n",
        "        prompt_templates = templates\n",
        "    elif args.prompt_template == \"face\":\n",
        "        prompt_templates = face_templates\n",
        "    else:\n",
        "        prompt_templates = art_templates\n",
        "    print(f\"Using the default {len(prompt_templates)} templates!\")\n",
        "else:\n",
        "    assert \"{placeholder_token}\" in args.prompt_template, \"You must specify the location of placeholder token by '{placeholder_token}'\"\n",
        "    prompt_templates = [args.prompt_template]\n",
        "# Get the text embedding for e4t conditioning\n",
        "encoder_hidden_states_for_e4t = text_encoder(input_ids_for_encoder.to(accelerator.device))[0].to(dtype=weight_dtype)\n",
        "\n",
        "try:\n",
        "    for epoch in range(first_epoch, args.num_train_epochs):\n",
        "        unet.train()\n",
        "        e4t_encoder.train()\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            # Skip steps until we reach the resumed step\n",
        "            if args.resume_from_checkpoint and epoch == first_epoch and step < resume_step:\n",
        "                if step % args.gradient_accumulation_steps == 0:\n",
        "                    progress_bar.update(1)\n",
        "                continue\n",
        "            with accelerator.accumulate(unet):\n",
        "                # Convert images to latent space\n",
        "                pixel_values = batch[\"pixel_values\"]\n",
        "                latents = vae.encode(pixel_values.to(dtype=weight_dtype)).latent_dist.sample().detach()\n",
        "                latents = latents * vae.config.scaling_factor\n",
        "\n",
        "                # Sample noise that we'll add to the latents\n",
        "                noise = torch.randn_like(latents)\n",
        "                bsz = latents.shape[0]\n",
        "                # Sample a random timestep for each image\n",
        "                timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device)\n",
        "                timesteps = timesteps.long()\n",
        "                \n",
        "                # get prompt\n",
        "                batch_templates = random.choices(prompt_templates, k=bsz)\n",
        "                prompt = [prompt_template.format(placeholder_token=args.placeholder_token) for prompt_template in batch_templates]\n",
        "                input_ids = tokenizer(\n",
        "                    prompt, padding=\"max_length\", truncation=True, max_length=tokenizer.model_max_length,\n",
        "                    return_tensors=\"pt\"\n",
        "                ).input_ids\n",
        "                # Get the text embedding\n",
        "                inputs_embeds = text_encoder.get_input_embeddings()(input_ids.to(accelerator.device))\n",
        "                placeholder_token_id_idxs = [i.index(placeholder_token_id) for i in input_ids.cpu().tolist()]\n",
        "\n",
        "                # Add noise to the latents according to the noise magnitude at each timestep\n",
        "                # (this is the forward diffusion process)\n",
        "                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
        "                encoder_hidden_states_for_e4t_forward = encoder_hidden_states_for_e4t.expand(bsz, -1, -1)\n",
        "                # Get the unet encoder outputs\n",
        "                encoder_outputs = unet(noisy_latents, timesteps, encoder_hidden_states_for_e4t_forward, return_encoder_outputs=True)\n",
        "                # Forward E4T encoder to get the embedding\n",
        "                domain_embed = e4t_encoder(x=pixel_values, unet_down_block_samples=encoder_outputs[\"down_block_samples\"])\n",
        "                # update word embedding\n",
        "                domain_embed = class_embed.clone().expand(bsz, -1) + args.domain_embed_scale * domain_embed\n",
        "                \n",
        "                for i, placeholder_token_id_idx in enumerate(placeholder_token_id_idxs):\n",
        "                    inputs_embeds[i, placeholder_token_id_idx, :] = domain_embed[i]\n",
        "                \n",
        "                # Get the text embedding for conditioning\n",
        "                encoder_hidden_states = text_encoder(inputs_embeds=inputs_embeds)[0].to(dtype=weight_dtype)\n",
        "                # Predict the noise residual\n",
        "                model_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
        "                # Get the target for loss depending on the prediction type\n",
        "                if noise_scheduler.config.prediction_type == \"epsilon\":\n",
        "                    target = noise\n",
        "                elif noise_scheduler.config.prediction_type == \"v_prediction\":\n",
        "                    target = noise_scheduler.get_velocity(latents, noise, timesteps)\n",
        "                else:\n",
        "                    raise ValueError(f\"Unknown prediction type {noise_scheduler.config.prediction_type}\")\n",
        "                # compute loss\n",
        "                loss_diff = F.mse_loss(model_pred.float(), target.float(), reduction=\"mean\")\n",
        "                loss_reg = args.reg_lambda * domain_embed.pow(2).sum()\n",
        "                loss = loss_diff + loss_reg\n",
        "                accelerator.backward(loss)\n",
        "                # if accelerator.sync_gradients:\n",
        "                #     params_to_clip = itertools.chain(unet.parameters(), e4t_encoder.parameters())\n",
        "                #     accelerator.clip_grad_norm_(params_to_clip, args.max_grad_norm)\n",
        "                optimizer.step()\n",
        "                lr_scheduler.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            # # Checks if the accelerator has performed an optimization step behind the scenes\n",
        "            # if accelerator.sync_gradients:\n",
        "            progress_bar.update(1)\n",
        "            global_step += 1\n",
        "            if global_step % args.checkpointing_steps == 0:\n",
        "                save_weights(global_step)\n",
        "                save_path = os.path.join(args.output_dir, f\"checkpoint-{global_step}\")\n",
        "                accelerator.save_state(save_path)\n",
        "                print(f\"Saved state to {save_path}\")\n",
        "            # log at first step\n",
        "            if global_step == 1 or global_step % args.log_steps == 0:\n",
        "                images = accelerator.gather(batch[\"pixel_values\"])\n",
        "                if accelerator.is_main_process:\n",
        "                    sample(images, global_step)\n",
        "\n",
        "            logs = {\n",
        "                \"train/loss\": loss.detach().item(),\n",
        "                \"train/loss_diff\": loss_diff.detach().item(),\n",
        "                \"train/loss_reg\": loss_reg.detach().item(),\n",
        "                \"train/lr\": lr_scheduler.get_last_lr()[0]\n",
        "            }\n",
        "            progress_bar.set_postfix(**logs)\n",
        "            accelerator.log(logs, step=global_step)\n",
        "\n",
        "            if global_step >= args.max_train_steps:\n",
        "                break\n",
        "except KeyboardInterrupt:\n",
        "    print(\"Summoning checkpoint...\")\n",
        "    pass\n",
        "accelerator.wait_for_everyone()\n",
        "save_weights(global_step)\n",
        "accelerator.end_training()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4fLD33qqA9vl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "outputId": "dee6a5ae-16a7-45a4-842e-8c77c4779d60"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-9bf658634962>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mepoch_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"/content/e4t-diffusion/pretrained-wikiart/{epoch}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mrepo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush_to_hub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommit_message\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"epoch {epoch}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
          ]
        }
      ],
      "source": [
        "import shutil\n",
        "epoch = \"70000\"\n",
        "epoch_dir = f\"/content/e4t-diffusion/pretrained-wikiart/{epoch}\"\n",
        "\n",
        "for file in os.listdir(epoch_dir):\n",
        "  shutil.copy(os.path.join(epoch_dir, file), checkpoint_dir)\n",
        "repo.push_to_hub(commit_message=f\"epoch {epoch}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0e5e18c42a4545a4bc46b04fbf07925e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "25ca156f7b9941009ac187bc3174b9d2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2cc85a5042c84988b18365e371a47e06": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "42a5063b02a34ac286dbf7d6d5282448": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c5f501869bb4cd78114cbc3d551513c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2cc85a5042c84988b18365e371a47e06",
            "placeholder": "",
            "style": "IPY_MODEL_d9fd0c54eb93406a9dcefe8df1d0f1e8",
            "value": " 3/3 [01:39&lt;00:00, 32.99s/it]"
          }
        },
        "4f600f841743448c88322a4002316f5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_feeb134461c6429d8a6c8cbb0a732953",
            "max": 100000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4f98069d1e924e10bba82dc7cabf40f8",
            "value": 704
          }
        },
        "4f98069d1e924e10bba82dc7cabf40f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "644bd4ef6df645e18c53e382431226f3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69dd3170dae14ed281f1973beead8080": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_42a5063b02a34ac286dbf7d6d5282448",
            "placeholder": "",
            "style": "IPY_MODEL_8edcf0e5d7ac496c8945bbd42f5b6008",
            "value": "Generating samples: 100%"
          }
        },
        "83e8730cdbb1457aa25238cc03d08ecd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_69dd3170dae14ed281f1973beead8080",
              "IPY_MODEL_b957906aa6aa476e895f9bc9a223a05e",
              "IPY_MODEL_4c5f501869bb4cd78114cbc3d551513c"
            ],
            "layout": "IPY_MODEL_a2017d8f67de41528628e6511107a3cc"
          }
        },
        "8edcf0e5d7ac496c8945bbd42f5b6008": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9ab5463c284e4ca8935a1bad31d7384d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2017d8f67de41528628e6511107a3cc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a81cbf0e261d434d8e8ac93a91b4ce64": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b957906aa6aa476e895f9bc9a223a05e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_644bd4ef6df645e18c53e382431226f3",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0e5e18c42a4545a4bc46b04fbf07925e",
            "value": 3
          }
        },
        "cc332469a6634e949b8a75602b3e3417": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cdb633a5f96645fbbb805b447d3fa59e",
            "placeholder": "",
            "style": "IPY_MODEL_a81cbf0e261d434d8e8ac93a91b4ce64",
            "value": "Steps:   0%"
          }
        },
        "cdb633a5f96645fbbb805b447d3fa59e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9fd0c54eb93406a9dcefe8df1d0f1e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e324fd6012c94facb7c3d80afb5af639": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cc332469a6634e949b8a75602b3e3417",
              "IPY_MODEL_4f600f841743448c88322a4002316f5d",
              "IPY_MODEL_ffa71bf2e2064a678c14e914de7bcd84"
            ],
            "layout": "IPY_MODEL_9ab5463c284e4ca8935a1bad31d7384d"
          }
        },
        "fa5bcd6c12b042648b853ce201eb73f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "feeb134461c6429d8a6c8cbb0a732953": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ffa71bf2e2064a678c14e914de7bcd84": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_25ca156f7b9941009ac187bc3174b9d2",
            "placeholder": "",
            "style": "IPY_MODEL_fa5bcd6c12b042648b853ce201eb73f4",
            "value": " 704/100000 [13:52&lt;26:07:39,  1.06it/s, train/loss=0.185, train/loss_diff=0.185, train/loss_reg=4.63e-5, train/lr=4e-6]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}